{
    "300": {
        "file_id": 10,
        "content": "        self.handle = text_attention_final.register_forward_hook(self._hook)\n        self.clip_normalize = preprocess.transforms[-1]\n        self.cleared = False\n    def find_layer(self,  layer):\n        modules = dict([*self.clip.named_modules()])\n        return modules.get(layer, None)\n    def clear(self):\n        if self.cleared:\n            return\n        self.handle()\n    def _hook(self, _, inputs, outputs):\n        self.text_encodings = outputs\n    @property\n    def dim_latent(self):\n        return self.dim_latent_\n    @property\n    def image_size(self):\n        return self.clip.visual.input_resolution\n    @property\n    def image_channels(self):\n        return 3\n    @property\n    def max_text_len(self):\n        return self.clip.context_length\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        is_eos_id = (text == self.eos_id)\n        text_mask_excluding_eos = is_eos_id.cumsum(dim = -1) == 0\n        text_mask = F.pad(text_mask_excluding_eos, (1, -1), value = True)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:320-360"
    },
    "301": {
        "file_id": 10,
        "content": "This code is part of a neural network model for text-to-image generation using PyTorch. It includes functions to handle text attention, clear the internal state, and embed input text. The class has properties such as `dim_latent`, `image_size`, `image_channels`, `max_text_len` which are used to define the network's structure and behavior.",
        "type": "comment"
    },
    "302": {
        "file_id": 10,
        "content": "        text_mask = text_mask & (text != 0)\n        assert not self.cleared\n        text_embed = self.clip.encode_text(text)\n        text_encodings = self.text_encodings\n        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        del self.text_encodings\n        return EmbeddedText(l2norm(text_embed.float()), text_encodings.float())\n    @torch.no_grad()\n    def embed_image(self, image):\n        assert not self.cleared\n        image = self.validate_and_resize_image(image)\n        image = self.clip_normalize(image)\n        image_embed = self.clip.encode_image(image)\n        return EmbeddedImage(l2norm(image_embed.float()), None)\nclass OpenClipAdapter(BaseClipAdapter):\n    def __init__(\n        self,\n        name = 'ViT-B/32',\n        pretrained = 'laion400m_e32'\n    ):\n        import open_clip\n        clip, _, preprocess = open_clip.create_model_and_transforms(name, pretrained = pretrained)\n        super().__init__(clip)\n        self.eos_id = 49407\n        text_attention_final = self.find_layer('ln_final')",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:361-390"
    },
    "303": {
        "file_id": 10,
        "content": "Method to embed text using CLIP model by encoding the input text, applying a mask on text encodings, and returning EmbeddedText object with L2 normalized text embedding and float text encodings.",
        "type": "comment"
    },
    "304": {
        "file_id": 10,
        "content": "        self._dim_latent = text_attention_final.weight.shape[0]\n        self.handle = text_attention_final.register_forward_hook(self._hook)\n        self.clip_normalize = preprocess.transforms[-1]\n        self.cleared = False\n    def find_layer(self,  layer):\n        modules = dict([*self.clip.named_modules()])\n        return modules.get(layer, None)\n    def clear(self):\n        if self.cleared:\n            return\n        self.handle()\n    def _hook(self, _, inputs, outputs):\n        self.text_encodings = outputs\n    @property\n    def dim_latent(self):\n        return self._dim_latent\n    @property\n    def image_size(self):\n        image_size = self.clip.visual.image_size\n        if isinstance(image_size, tuple):\n            return max(image_size)\n        return image_size\n    @property\n    def image_channels(self):\n        return 3\n    @property\n    def max_text_len(self):\n        return self.clip.context_length\n    @torch.no_grad()\n    def embed_text(self, text):\n        text = text[..., :self.max_text_len]\n        is_eos_id = (text == self.eos_id)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:391-433"
    },
    "305": {
        "file_id": 10,
        "content": "The code represents a class that appears to be a part of a larger model. It has methods for embedding text, clearing internal state, finding layers in the network, and retrieving properties like latent dimension and maximum text length. The class relies on other components such as `preprocess`, `clip`, and `image_size`.",
        "type": "comment"
    },
    "306": {
        "file_id": 10,
        "content": "        text_mask_excluding_eos = is_eos_id.cumsum(dim = -1) == 0\n        text_mask = F.pad(text_mask_excluding_eos, (1, -1), value = True)\n        text_mask = text_mask & (text != 0)\n        assert not self.cleared\n        text_embed = self.clip.encode_text(text)\n        text_encodings = self.text_encodings\n        text_encodings = text_encodings.masked_fill(~text_mask[..., None], 0.)\n        del self.text_encodings\n        return EmbeddedText(l2norm(text_embed.float()), text_encodings.float())\n    @torch.no_grad()\n    def embed_image(self, image):\n        assert not self.cleared\n        image = self.validate_and_resize_image(image)\n        image = self.clip_normalize(image)\n        image_embed = self.clip.encode_image(image)\n        return EmbeddedImage(l2norm(image_embed.float()), None)\n# classifier free guidance functions\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:434-459"
    },
    "307": {
        "file_id": 10,
        "content": "This function takes in a text input and returns an EmbeddedText object containing the embedded text representation and a corresponding mask. It first creates a mask excluding the end of sentence (EOS) token, pads it, and applies the mask to the original mask. Then, it encodes the text using CLIP's encode_text function, and finally normalizes the resulting embeddings. The classifier free guidance functions return a probability mask based on the given probability value for a specific shape and device.",
        "type": "comment"
    },
    "308": {
        "file_id": 10,
        "content": "    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n# gaussian diffusion helper functions\ndef extract(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\ndef meanflat(x):\n    return x.mean(dim = tuple(range(1, len(x.shape))))\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    return 0.5 * (-1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2))\ndef approx_standard_normal_cdf(x):\n    return 0.5 * (1.0 + torch.tanh(((2.0 / math.pi) ** 0.5) * (x + 0.044715 * (x ** 3))))\ndef discretized_gaussian_log_likelihood(x, *, means, log_scales, thres = 0.999):\n    assert x.shape == means.shape == log_scales.shape\n    # attempting to correct nan gradients when learned variance is turned on\n    # in the setting of deepspeed fp16\n    eps = 1e-12 if x.dtype == torch.float32 else 1e-3\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1. / 255.)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:460-488"
    },
    "309": {
        "file_id": 10,
        "content": "This code defines several helper functions used in the DALLE2-pytorch model. These functions are involved in tasks such as extracting values, calculating normal KL divergence, approximating the standard normal cumulative distribution function, and computing the discretized Gaussian log likelihood. The code also includes error handling for potential nan gradients when using deepspeed fp16.",
        "type": "comment"
    },
    "310": {
        "file_id": 10,
        "content": "    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1. / 255.)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = log(cdf_plus, eps = eps)\n    log_one_minus_cdf_min = log(1. - cdf_min, eps = eps)\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(x < -thres,\n        log_cdf_plus,\n        torch.where(x > thres,\n            log_one_minus_cdf_min,\n            log(cdf_delta, eps = eps)))\n    return log_probs\ndef cosine_beta_schedule(timesteps, s = 0.008):\n    \"\"\"\n    cosine schedule\n    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n    \"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / first(alphas_cumprod)\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clip(betas, 0, 0.999)\ndef linear_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:489-519"
    },
    "311": {
        "file_id": 10,
        "content": "Function at line 488-518 calculates log probabilities for a given input x, using an adaptive quantile regression approach with a cosine or linear schedule. The cosine_beta_schedule function generates a sequence of beta values using a cosine schedule, and the linear_beta_schedule function generates a sequence of beta values linearly.",
        "type": "comment"
    },
    "312": {
        "file_id": 10,
        "content": "    beta_end = scale * 0.02\n    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\ndef quadratic_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001\n    beta_end = scale * 0.02\n    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps, dtype = torch.float64) ** 2\ndef sigmoid_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001\n    beta_end = scale * 0.02\n    betas = torch.linspace(-6, 6, timesteps, dtype = torch.float64)\n    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\nclass NoiseScheduler(nn.Module):\n    def __init__(self, *, beta_schedule, timesteps, loss_type, p2_loss_weight_gamma = 0., p2_loss_weight_k = 1):\n        super().__init__()\n        if beta_schedule == \"cosine\":\n            betas = cosine_beta_schedule(timesteps)\n        elif beta_schedule == \"linear\":\n            betas = linear_beta_schedule(timesteps)\n        elif beta_schedule == \"quadratic\":\n            betas = quadratic_beta_schedule(timesteps)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:520-548"
    },
    "313": {
        "file_id": 10,
        "content": "This code defines three beta scheduling functions (linear, quadratic, cosine) and a class for the NoiseScheduler. The scheduler initializes with a selected beta schedule and timesteps. The beta_schedule parameter determines which function to use for generating the betas, which represent noise scaling factors in the model's training process.",
        "type": "comment"
    },
    "314": {
        "file_id": 10,
        "content": "        elif beta_schedule == \"jsd\":\n            betas = 1.0 / torch.linspace(timesteps, 1, timesteps)\n        elif beta_schedule == \"sigmoid\":\n            betas = sigmoid_beta_schedule(timesteps)\n        else:\n            raise NotImplementedError()\n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, axis = 0)\n        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n        # register buffer helper function to cast double back to float\n        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n        register_buffer('betas', betas)\n        register_buffer('alphas_cumprod', alphas_cumprod)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:549-580"
    },
    "315": {
        "file_id": 10,
        "content": "This code sets the beta schedule and alpha values based on user input, then selects a loss function according to the specified type. The code also registers buffer helper functions for 'betas' and 'alphas_cumprod'.",
        "type": "comment"
    },
    "316": {
        "file_id": 10,
        "content": "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        register_buffer('posterior_variance', posterior_variance)\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:581-601"
    },
    "317": {
        "file_id": 10,
        "content": "The code is registering various buffers for computations related to diffusion. It calculates the posterior variance and clips the log of the posterior variance to avoid numerical instability at the beginning of the diffusion chain.",
        "type": "comment"
    },
    "318": {
        "file_id": 10,
        "content": "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n        # p2 loss reweighting\n        self.has_p2_loss_reweighting = p2_loss_weight_gamma > 0.\n        register_buffer('p2_loss_weight', (p2_loss_weight_k + alphas_cumprod / (1 - alphas_cumprod)) ** -p2_loss_weight_gamma)\n    def sample_random_times(self, batch):\n        return torch.randint(0, self.num_timesteps, (batch,), device = self.betas.device, dtype = torch.long)\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:602-620"
    },
    "319": {
        "file_id": 10,
        "content": "In this code segment, the author is computing posterior means for a model, performing loss reweighting, generating random times, and calculating posterior values. The posterior means are calculated based on betas and alphas, while the loss reweighting considers p2_loss_weight_gamma. Random times are sampled for a batch of inputs using torch.randint. The q_posterior function calculates posterior mean, variance, and log-variance clipped from these computed values.",
        "type": "comment"
    },
    "320": {
        "file_id": 10,
        "content": "    def q_sample(self, x_start, t, noise = None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n    def calculate_v(self, x_start, t, noise = None):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n        )\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape = x_from.shape\n        noise = default(noise, lambda: torch.randn_like(x_from))\n        alpha = extract(self.sqrt_alphas_cumprod, from_t, shape)\n        sigma = extract(self.sqrt_one_minus_alphas_cumprod, from_t, shape)\n        alpha_next = extract(self.sqrt_alphas_cumprod, to_t, shape)\n        sigma_next = extract(self.sqrt_one_minus_alphas_cumprod, to_t, shape)\n        return x_from * (alpha_next / alpha) + noise * (sigma_next * alpha - sigma * alpha_next) / alpha",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:622-645"
    },
    "321": {
        "file_id": 10,
        "content": "The code defines three functions: `q_sample`, `calculate_v`, and `q_sample_from_to`. These functions are part of a neural network for generating images. `q_sample` combines alpha and noise values to generate a sample, while `calculate_v` calculates the difference between an alpha-blended noise and a one minus alpha-blended image start. The `q_sample_from_to` function samples from one timestep to another by interpolating alphas and sigmas.",
        "type": "comment"
    },
    "322": {
        "file_id": 10,
        "content": "    def predict_start_from_v(self, x_t, t, v):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n        )\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n    def predict_noise_from_start(self, x_t, t, x0):\n        return (\n            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n        )\n    def p2_reweigh_loss(self, loss, times):\n        if not self.has_p2_loss_reweighting:\n            return loss\n        return loss * extract(self.p2_loss_weight, times, loss.shape)\n# rearrange image to sequence\nclass RearrangeToSequence(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:647-677"
    },
    "323": {
        "file_id": 10,
        "content": "The code defines three methods for predicting values from different inputs, including v and noise. It also includes a method to reweight loss using p2_loss_weight and a class to rearrange images into sequences.",
        "type": "comment"
    },
    "324": {
        "file_id": 10,
        "content": "        x = rearrange(x, 'b c ... -> b ... c')\n        x, ps = pack([x], 'b * c')\n        x = self.fn(x)\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b ... c -> b c ...')\n        return x\n# diffusion prior\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5, fp16_eps = 1e-3, stable = False):\n        super().__init__()\n        self.eps = eps\n        self.fp16_eps = fp16_eps\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        eps = self.eps if x.dtype == torch.float32 else self.fp16_eps\n        if self.stable:\n            x = x / x.amax(dim = -1, keepdim = True).detach()\n        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = -1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5, fp16_eps = 1e-3, stable = False):\n        super().__init__()\n        self.eps = eps\n        self.fp16_eps = fp16_eps",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:678-711"
    },
    "325": {
        "file_id": 10,
        "content": "This function is applying layer normalization to input tensor 'x' and returning the normalized output. The 'LayerNorm' class is a type of layer normalization, while 'ChanLayerNorm' is a channel-wise version. The code includes settings for epsilon, float precision, and stability options.",
        "type": "comment"
    },
    "326": {
        "file_id": 10,
        "content": "        self.stable = stable\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n    def forward(self, x):\n        eps = self.eps if x.dtype == torch.float32 else self.fp16_eps\n        if self.stable:\n            x = x / x.amax(dim = 1, keepdim = True).detach()\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n# mlp\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        *,\n        expansion_factor = 2.,\n        depth = 2,\n        norm = False,\n    ):\n        super().__init__()\n        hidden_dim = int(expansion_factor * dim_out)\n        norm_fn = lambda: nn.LayerNorm(hidden_dim) if norm else nn.Identity()\n        layers = [nn.Sequential(\n            nn.Linear(dim_in, hidden_dim),",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:712-750"
    },
    "327": {
        "file_id": 10,
        "content": "This code defines a Residual class that wraps a function and adds it to the input. It also contains an MLP (Multi-Layer Perceptron) class with optional normalization and activation functions, followed by a series of fully connected layers. The forward method in DALLE2_PyTorch performs normalization, calculates mean and variance, then applies element-wise transformations before returning the output.",
        "type": "comment"
    },
    "328": {
        "file_id": 10,
        "content": "            nn.SiLU(),\n            norm_fn()\n        )]\n        for _ in range(depth - 1):\n            layers.append(nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.SiLU(),\n                norm_fn()\n            ))\n        layers.append(nn.Linear(hidden_dim, dim_out))\n        self.net = nn.Sequential(*layers)\n    def forward(self, x):\n        return self.net(x.float())\n# relative positional bias for causal transformer\nclass RelPosBias(nn.Module):\n    def __init__(\n        self,\n        heads = 8,\n        num_buckets = 32,\n        max_distance = 128,\n    ):\n        super().__init__()\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n    @staticmethod\n    def _relative_position_bucket(\n        relative_position,\n        num_buckets = 32,\n        max_distance = 128\n    ):\n        n = -relative_position\n        n = torch.max(n, torch.zeros_like(n))\n        max_exact = num_buckets // 2\n        is_small = n < max_exact",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:751-792"
    },
    "329": {
        "file_id": 10,
        "content": "This code defines a neural network architecture for the DALL-E 2 model. It includes a sequential layer with multiple linear layers, SiLU activation function, and normalization. The forward method performs inference on input data. Another class is defined for relative positional bias in causal transformer. The RelPosBias class initializes an embedding layer to calculate the relative position between elements for attention mechanism. It uses the concept of buckets, where each bucket represents a range of distances between two elements, and computes the relative position bucket based on input data.",
        "type": "comment"
    },
    "330": {
        "file_id": 10,
        "content": "        val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n        return torch.where(is_small, n, val_if_large)\n    def forward(self, i, j, *, device):\n        q_pos = torch.arange(i, dtype = torch.long, device = device)\n        k_pos = torch.arange(j, dtype = torch.long, device = device)\n        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n        rp_bucket = self._relative_position_bucket(rel_pos, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        return rearrange(values, 'i j h -> h i j')\n# feedforward\nclass SwiGLU(nn.Module):\n    \"\"\" used successfully in https://arxiv.org/abs/2204.0231 \"\"\"\n    def forward(self, x):\n        x, gate = x.chunk(2, dim = -1)\n        return x * F.silu(gate)\ndef FeedForward(\n    dim,\n    mult = 4,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:794-816"
    },
    "331": {
        "file_id": 10,
        "content": "This code snippet defines a class for DALLE2-pytorch, containing a method to calculate relative position buckets and an attention layer. The attention layer uses the SwiGLU activation function in its FeedForward module. The purpose of this code is to facilitate the calculation and application of positional embeddings in a transformer model.",
        "type": "comment"
    },
    "332": {
        "file_id": 10,
        "content": "    dropout = 0.,\n    post_activation_norm = False\n):\n    \"\"\" post-activation norm https://arxiv.org/abs/2110.09456 \"\"\"\n    inner_dim = int(mult * dim)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, inner_dim * 2, bias = False),\n        SwiGLU(),\n        LayerNorm(inner_dim) if post_activation_norm else nn.Identity(),\n        nn.Dropout(dropout),\n        nn.Linear(inner_dim, dim, bias = False)\n    )\n# attention\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.,\n        causal = False,\n        rotary_emb = None,\n        cosine_sim = True,\n        cosine_sim_scale = 16\n    ):\n        super().__init__()\n        self.scale = cosine_sim_scale if cosine_sim else (dim_head ** -0.5)\n        self.cosine_sim = cosine_sim\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.causal = causal\n        self.norm = LayerNorm(dim)\n        self.dropout = nn.Dropout(dropout)\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:817-858"
    },
    "333": {
        "file_id": 10,
        "content": "The code defines a module that applies post-activation normalization. It also includes a nested Attention class that performs multi-head attention with optional causal masking and rotary embedding. The main components include layer normalization, dropout regularization, and linear transformations for dimensionality adjustments. The cosine similarity calculation is utilized if specified.",
        "type": "comment"
    },
    "334": {
        "file_id": 10,
        "content": "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n        self.rotary_emb = rotary_emb\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n    def forward(self, x, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n        x = self.norm(x)\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n        q = q * self.scale\n        # rotary embeddings\n        if exists(self.rotary_emb):\n            q, k = map(self.rotary_emb.rotate_queries_or_keys, (q, k))\n        # add null key / value for classifier free guidance in prior net\n        nk, nv = map(lambda t: repeat(t, 'd -> b 1 d', b = b), self.null_kv.unbind(dim = -2))\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n        # whether to use cosine sim\n        if self.cosine_sim:",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:859-891"
    },
    "335": {
        "file_id": 10,
        "content": "This code defines a self-attention layer for DALL·E 2, initializing linear layers and including the option to use rotary embeddings. It also allows for classifier free guidance by adding null key/value pairs and using cosine similarity if enabled.",
        "type": "comment"
    },
    "336": {
        "file_id": 10,
        "content": "            q, k = map(l2norm, (q, k))\n        q, k = map(lambda t: t * math.sqrt(self.scale), (q, k))\n        # calculate query / key similarities\n        sim = einsum('b h i d, b j d -> b h i j', q, k)\n        # relative positional encoding (T5 style)\n        if exists(attn_bias):\n            sim = sim + attn_bias\n        # masking\n        max_neg_value = -torch.finfo(sim.dtype).max\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), dtype = torch.bool, device = device).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n        # attention\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.type(sim.dtype)\n        attn = self.dropout(attn)\n        # aggregate values\n        out = einsum('b h i j, b j d -> b h i d', attn, v)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:892-928"
    },
    "337": {
        "file_id": 10,
        "content": "This code snippet performs multi-head attention by first normalizing the query and key tensors, calculating their similarities, adding relative positional encoding if available, masking irrelevant values based on a given mask, applying causal masking if specified, and finally computing the attention weights and aggregating the corresponding values.",
        "type": "comment"
    },
    "338": {
        "file_id": 10,
        "content": "        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\nclass CausalTransformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        ff_mult = 4,\n        norm_in = False,\n        norm_out = True,\n        attn_dropout = 0.,\n        ff_dropout = 0.,\n        final_proj = True,\n        normformer = False,\n        rotary_emb = True\n    ):\n        super().__init__()\n        self.init_norm = LayerNorm(dim) if norm_in else nn.Identity() # from latest BLOOM model and Yandex's YaLM\n        self.rel_pos_bias = RelPosBias(heads = heads)\n        rotary_emb = RotaryEmbedding(dim = min(32, dim_head)) if rotary_emb else None\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, causal = True, dim_head = dim_head, heads = heads, dropout = attn_dropout, rotary_emb = rotary_emb),\n                FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, post_activation_norm = normformer)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:930-961"
    },
    "339": {
        "file_id": 10,
        "content": "This code defines a `CausalTransformer` class for natural language processing tasks. The class initializes several modules such as LayerNorm, RelPosBias, RotaryEmbedding, and Attention. It also includes a FeedForward layer with configurable parameters like `dim`, `depth`, `dim_head`, `heads`, `ff_mult`, `attn_dropout`, `ff_dropout`, `norm_in`, `norm_out`, `final_proj`, and `rotary_emb`. The code snippet you provided is responsible for rearranging the tensor dimensions and returning it after processing by the `CausalTransformer` model.",
        "type": "comment"
    },
    "340": {
        "file_id": 10,
        "content": "            ]))\n        self.norm = LayerNorm(dim, stable = True) if norm_out else nn.Identity()  # unclear in paper whether they projected after the classic layer norm for the final denoised image embedding, or just had the transformer output it directly: plan on offering both options\n        self.project_out = nn.Linear(dim, dim, bias = False) if final_proj else nn.Identity()\n    def forward(self, x):\n        n, device = x.shape[1], x.device\n        x = self.init_norm(x)\n        attn_bias = self.rel_pos_bias(n, n + 1, device = device)\n        for attn, ff in self.layers:\n            x = attn(x, attn_bias = attn_bias) + x\n            x = ff(x) + x\n        out = self.norm(x)\n        return self.project_out(out)\nclass DiffusionPriorNetwork(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_timesteps = None,\n        num_time_embeds = 1,\n        num_image_embeds = 1,\n        num_text_embeds = 1,\n        max_text_len = 256,\n        self_cond = False,\n        **kwargs\n    ):\n        super().__init__()",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:962-993"
    },
    "341": {
        "file_id": 10,
        "content": "The code initializes a DiffusionPriorNetwork model with multiple layers, including attention and feed-forward modules. It also includes layer normalization and the option to project the output. The network takes in input of varying dimensions and can condition on time, image, and/or text embeddings. The self_cond parameter determines whether or not to use self-conditioning.",
        "type": "comment"
    },
    "342": {
        "file_id": 10,
        "content": "        self.dim = dim\n        self.num_time_embeds = num_time_embeds\n        self.num_image_embeds = num_image_embeds\n        self.num_text_embeds = num_text_embeds\n        self.to_text_embeds = nn.Sequential(\n            nn.Linear(dim, dim * num_text_embeds) if num_text_embeds > 1 else nn.Identity(),\n            Rearrange('b (n d) -> b n d', n = num_text_embeds)\n        )\n        self.continuous_embedded_time = not exists(num_timesteps)\n        self.to_time_embeds = nn.Sequential(\n            nn.Embedding(num_timesteps, dim * num_time_embeds) if exists(num_timesteps) else nn.Sequential(SinusoidalPosEmb(dim), MLP(dim, dim * num_time_embeds)), # also offer a continuous version of timestep embeddings, with a 2 layer MLP\n            Rearrange('b (n d) -> b n d', n = num_time_embeds)\n        )\n        self.to_image_embeds = nn.Sequential(\n            nn.Linear(dim, dim * num_image_embeds) if num_image_embeds > 1 else nn.Identity(),\n            Rearrange('b (n d) -> b n d', n = num_image_embeds)\n        )\n        self.learned_query = nn.Parameter(torch.randn(dim))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:994-1017"
    },
    "343": {
        "file_id": 10,
        "content": "This code defines a class with parameters for dimensionality, number of time, image, and text embeddings. It initializes layers to transform input into text, time, and image embeddings. The \"learned_query\" is a learned parameter for the model.",
        "type": "comment"
    },
    "344": {
        "file_id": 10,
        "content": "        self.causal_transformer = CausalTransformer(dim = dim, **kwargs)\n        # dalle1 learned padding strategy\n        self.max_text_len = max_text_len\n        self.null_text_encodings = nn.Parameter(torch.randn(1, max_text_len, dim))\n        self.null_text_embeds = nn.Parameter(torch.randn(1, num_text_embeds, dim))\n        self.null_image_embed = nn.Parameter(torch.randn(1, dim))\n        # whether to use self conditioning, Hinton's group's new ddpm technique\n        self.self_cond = self_cond\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n        if cond_scale == 1:\n            return logits\n        null_logits = self.forward(*args, text_cond_drop_prob = 1., image_cond_drop_prob = 1, **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n    def forward(\n        self,\n        image_embed,\n        diffusion_timesteps,\n        *,\n        text_embed,\n        text_encodings = None,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1018-1052"
    },
    "345": {
        "file_id": 10,
        "content": "The code defines a model with a causal transformer and includes parameters for padding strategy, self-conditioning, and a function to perform forward calculations. The `forward_with_cond_scale` method takes conditional scaling as input and returns the scaled logits by combining original logits with null logits at 100% condition drop probabilities.",
        "type": "comment"
    },
    "346": {
        "file_id": 10,
        "content": "        self_cond = None,\n        text_cond_drop_prob = 0.,\n        image_cond_drop_prob = 0.\n    ):\n        batch, dim, device, dtype = *image_embed.shape, image_embed.device, image_embed.dtype\n        num_time_embeds, num_image_embeds, num_text_embeds = self.num_time_embeds, self.num_image_embeds, self.num_text_embeds\n        # setup self conditioning\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros(batch, self.dim, device = device, dtype = dtype))\n            self_cond = rearrange(self_cond, 'b d -> b 1 d')\n        # in section 2.2, last paragraph\n        # \"... consisting of encoded text, CLIP text embedding, diffusion timestep embedding, noised CLIP image embedding, final embedding for prediction\"\n        text_embed = self.to_text_embeds(text_embed)\n        image_embed = self.to_image_embeds(image_embed)\n        # classifier free guidance masks\n        text_keep_mask = prob_mask_like((batch,), 1 - text_cond_drop_prob, device = device)\n        text_keep_mask = rearrange(text_keep_mask, 'b -> b 1 1')",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1053-1076"
    },
    "347": {
        "file_id": 10,
        "content": "This code initializes a model's parameters based on the given image_embed. It sets up self-conditioning if necessary, converts text and image embeddings to the appropriate format, and creates classifier free guidance masks for both text and image inputs. The model will use these embeddings and masks for prediction.",
        "type": "comment"
    },
    "348": {
        "file_id": 10,
        "content": "        image_keep_mask = prob_mask_like((batch,), 1 - image_cond_drop_prob, device = device)\n        image_keep_mask = rearrange(image_keep_mask, 'b -> b 1 1')\n        # make text encodings optional\n        # although the paper seems to suggest it is present <--\n        if not exists(text_encodings):\n            text_encodings = torch.empty((batch, 0, dim), device = device, dtype = dtype)\n        mask = torch.any(text_encodings != 0., dim = -1)\n        # replace any padding in the text encodings with learned padding tokens unique across position\n        text_encodings = text_encodings[:, :self.max_text_len]\n        mask = mask[:, :self.max_text_len]\n        text_len = text_encodings.shape[-2]\n        remainder = self.max_text_len - text_len\n        if remainder > 0:\n            text_encodings = F.pad(text_encodings, (0, 0, 0, remainder), value = 0.)\n            mask = F.pad(mask, (0, remainder), value = False)\n        # mask out text encodings with null encodings\n        null_text_encodings = self.null_text_encodings.to(text_encodings.dtype)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1078-1103"
    },
    "349": {
        "file_id": 10,
        "content": "This code snippet is preparing the input data for a DALL-E 2 model by handling text encodings. It creates an image_keep_mask, makes text encodings optional based on their existence, applies masking to remove padding or null encodings, and ensures that the length of text_encodings matches the expected maximum length.",
        "type": "comment"
    },
    "350": {
        "file_id": 10,
        "content": "        text_encodings = torch.where(\n            rearrange(mask, 'b n -> b n 1').clone() & text_keep_mask,\n            text_encodings,\n            null_text_encodings\n        )\n        # mask out text embeddings with null text embeddings\n        null_text_embeds = self.null_text_embeds.to(text_embed.dtype)\n        text_embed = torch.where(\n            text_keep_mask,\n            text_embed,\n            null_text_embeds\n        )\n        # mask out image embeddings with null image embeddings\n        null_image_embed = self.null_image_embed.to(image_embed.dtype)\n        image_embed = torch.where(\n            image_keep_mask,\n            image_embed,\n            null_image_embed\n        )\n        # whether text embedding is used for conditioning depends on whether text encodings are available for attention (for classifier free guidance, even though it seems from the paper it was not used in the prior ddpm, as the objective is different)\n        # but let's just do it right\n        if self.continuous_embedded_time:",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1105-1134"
    },
    "351": {
        "file_id": 10,
        "content": "This code section is applying masking to text, image, and null embeddings based on the `text_keep_mask` and `image_keep_mask`. It uses these masks to decide which embeddings to keep or replace with null embeddings. The embeddings are also being converted to appropriate data types. Additionally, there's a conditional check for continuous embedded time.",
        "type": "comment"
    },
    "352": {
        "file_id": 10,
        "content": "            diffusion_timesteps = diffusion_timesteps.type(dtype)\n        time_embed = self.to_time_embeds(diffusion_timesteps)\n        learned_queries = repeat(self.learned_query, 'd -> b 1 d', b = batch)\n        if self.self_cond:\n            learned_queries = torch.cat((self_cond, learned_queries), dim = -2)\n        tokens = torch.cat((\n            text_encodings,\n            text_embed,\n            time_embed,\n            image_embed,\n            learned_queries\n        ), dim = -2)\n        # attend\n        tokens = self.causal_transformer(tokens)\n        # get learned query, which should predict the image embedding (per DDPM timestep)\n        pred_image_embed = tokens[..., -1, :]\n        return pred_image_embed\nclass DiffusionPrior(nn.Module):\n    def __init__(\n        self,\n        net,\n        *,\n        clip = None,\n        image_embed_dim = None,\n        image_size = None,\n        image_channels = 3,\n        timesteps = 1000,\n        sample_timesteps = None,\n        cond_drop_prob = 0.,\n        text_cond_drop_prob = None,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1135-1174"
    },
    "353": {
        "file_id": 10,
        "content": "The code defines a DiffusionPrior class that takes in various inputs such as text encodings, timesteps, and image embeddings. It applies causal transformer to learn the learned_query, which predicts the image embedding per DDPM timestep. The text_cond_drop_prob parameter is optional and if provided, will dropout the text conditioning with a specified probability.",
        "type": "comment"
    },
    "354": {
        "file_id": 10,
        "content": "        image_cond_drop_prob = None,\n        loss_type = \"l2\",\n        predict_x_start = True,\n        predict_v = False,\n        beta_schedule = \"cosine\",\n        condition_on_text_encodings = True,  # the paper suggests this is needed, but you can turn it off for your CLIP preprocessed text embed -> image embed training\n        sampling_clamp_l2norm = False,       # whether to l2norm clamp the image embed at each denoising iteration (analogous to -1 to 1 clipping for usual DDPMs)\n        sampling_final_clamp_l2norm = False, # whether to l2norm the final image embedding output (this is also done for images in ddpm)\n        training_clamp_l2norm = False,\n        init_image_embed_l2norm = False,\n        image_embed_scale = None,            # this is for scaling the l2-normed image embedding, so it is more suitable for gaussian diffusion, as outlined by Katherine (@crowsonkb) https://github.com/lucidrains/DALLE2-pytorch/issues/60#issue-1226116132\n        clip_adapter_overrides = dict()\n    ):\n        super().__init__()",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1175-1188"
    },
    "355": {
        "file_id": 10,
        "content": "This code snippet initializes a DALLE2 model with various optional parameters for training and sampling. These include loss type, conditioning on text encodings, clamping of image embeddings, scaling the L2-normed image embedding, and adapter overrides for CLIP adapter integration.",
        "type": "comment"
    },
    "356": {
        "file_id": 10,
        "content": "        self.sample_timesteps = sample_timesteps\n        self.noise_scheduler = NoiseScheduler(\n            beta_schedule = beta_schedule,\n            timesteps = timesteps,\n            loss_type = loss_type\n        )\n        if exists(clip):\n            assert image_channels == clip.image_channels, f'channels of image ({image_channels}) should be equal to the channels that CLIP accepts ({clip.image_channels})'\n            if isinstance(clip, CLIP):\n                clip = XClipAdapter(clip, **clip_adapter_overrides)\n            elif isinstance(clip, CoCa):\n                clip = CoCaAdapter(clip, **clip_adapter_overrides)\n            assert isinstance(clip, BaseClipAdapter)\n            freeze_model_and_make_eval_(clip)\n            self.clip = clip\n        else:\n            assert exists(image_embed_dim), 'latent dimension must be given, if training prior network without CLIP given'\n            self.clip = None\n        self.net = net\n        self.image_embed_dim = default(image_embed_dim, lambda: clip.dim_latent)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1190-1214"
    },
    "357": {
        "file_id": 10,
        "content": "The code is initializing an instance of a model. It sets the sample_timesteps, creates a NoiseScheduler object with specified parameters, checks if CLIP is provided and adapts it if necessary, sets the image_embed_dim if not given, and assigns the network architecture (net) to be used.",
        "type": "comment"
    },
    "358": {
        "file_id": 10,
        "content": "        assert net.dim == self.image_embed_dim, f'your diffusion prior network has a dimension of {net.dim}, but you set your image embedding dimension (keyword image_embed_dim) on DiffusionPrior to {self.image_embed_dim}'\n        assert not exists(clip) or clip.dim_latent == self.image_embed_dim, f'you passed in a CLIP to the diffusion prior with latent dimensions of {clip.dim_latent}, but your image embedding dimension (keyword image_embed_dim) for the DiffusionPrior was set to {self.image_embed_dim}'\n        self.channels = default(image_channels, lambda: clip.image_channels)\n        self.text_cond_drop_prob = default(text_cond_drop_prob, cond_drop_prob)\n        self.image_cond_drop_prob = default(image_cond_drop_prob, cond_drop_prob)\n        self.can_classifier_guidance = self.text_cond_drop_prob > 0. and self.image_cond_drop_prob > 0.\n        self.condition_on_text_encodings = condition_on_text_encodings\n        # in paper, they do not predict the noise, but predict x0 directly for image embedding, claiming empirically better results. I'll just offer both.",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1216-1227"
    },
    "359": {
        "file_id": 10,
        "content": "The code asserts that the diffusion prior network dimension and the image embedding dimension are consistent, and checks if a CLIP is passed in with correct latent dimensions. It also sets channels, text conditional drop probability, image conditional drop probability, enables classifier guidance if probabilities are greater than 0, and conditions on text encodings. It offers both options to predict noise or x0 directly for image embedding as per the paper's claim of better results.",
        "type": "comment"
    },
    "360": {
        "file_id": 10,
        "content": "        self.predict_x_start = predict_x_start\n        self.predict_v = predict_v # takes precedence over predict_x_start\n        # @crowsonkb 's suggestion - https://github.com/lucidrains/DALLE2-pytorch/issues/60#issue-1226116132\n        self.image_embed_scale = default(image_embed_scale, self.image_embed_dim ** 0.5)\n        # whether to force an l2norm, similar to clipping denoised, when sampling\n        self.sampling_clamp_l2norm = sampling_clamp_l2norm\n        self.sampling_final_clamp_l2norm = sampling_final_clamp_l2norm\n        self.training_clamp_l2norm = training_clamp_l2norm\n        self.init_image_embed_l2norm = init_image_embed_l2norm\n        # device tracker\n        self.register_buffer('_dummy', torch.tensor([True]), persistent = False)\n    @property\n    def device(self):\n        return self._dummy.device\n    def l2norm_clamp_embed(self, image_embed):\n        return l2norm(image_embed) * self.image_embed_scale\n    def p_mean_variance(self, x, t, text_cond, self_cond = None, clip_denoised = False, cond_scale = 1.):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1229-1255"
    },
    "361": {
        "file_id": 10,
        "content": "The code sets various parameters and properties for an object, including predict_x_start, image_embed_scale, sampling_clamp_l2norm, etc. It also defines the l2norm_clamp_embed function and p_mean_variance function. The device property retrieves the device used by the object, and there's a register_buffer for tracking device usage.",
        "type": "comment"
    },
    "362": {
        "file_id": 10,
        "content": "        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'the model was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n        pred = self.net.forward_with_cond_scale(x, t, cond_scale = cond_scale, self_cond = self_cond, **text_cond)\n        if self.predict_v:\n            x_start = self.noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        elif self.predict_x_start:\n            x_start = pred\n        else:\n            x_start = self.noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        if clip_denoised and not self.predict_x_start:\n            x_start.clamp_(-1., 1.)\n        if self.predict_x_start and self.sampling_clamp_l2norm:\n            x_start = l2norm(x_start) * self.image_embed_scale\n        model_mean, posterior_variance, posterior_log_variance = self.noise_scheduler.q_posterior(x_start=x_start, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance, x_start",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1256-1274"
    },
    "363": {
        "file_id": 10,
        "content": "This code asserts that the model was not trained with conditional dropout, preventing classifier free guidance if cond_scale is anything other than 1. It then calculates and returns the model mean, posterior variance, posterior log variance, and x_start depending on different conditions.",
        "type": "comment"
    },
    "364": {
        "file_id": 10,
        "content": "    @torch.no_grad()\n    def p_sample(self, x, t, text_cond = None, self_cond = None, clip_denoised = True, cond_scale = 1.):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = t, text_cond = text_cond, self_cond = self_cond, clip_denoised = clip_denoised, cond_scale = cond_scale)\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n    @torch.no_grad()\n    def p_sample_loop_ddpm(self, shape, text_cond, cond_scale = 1.):\n        batch, device = shape[0], self.device\n        image_embed = torch.randn(shape, device = device)\n        x_start = None # for self-conditioning\n        if self.init_image_embed_l2norm:\n            image_embed = l2norm(image_embed) * self.image_embed_scale\n        for i in tqdm(reversed(range(",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1276-1296"
    },
    "365": {
        "file_id": 10,
        "content": "This code defines the `p_sample` and `p_sample_loop_ddpm` functions. `p_sample` takes input, generates a model mean and log variance, applies noise based on whether t is zero or not, and returns the prediction and x_start. `p_sample_loop_ddpm` initializes an image embedding, optionally normalizes it, and iterates through a reversed range to perform some unspecified operation for each iteration. The code uses PyTorch's `@torch.no_grad()` decorator to disable gradient computation during these functions' execution.",
        "type": "comment"
    },
    "366": {
        "file_id": 10,
        "content": "0, self.noise_scheduler.num_timesteps)), desc='sampling loop time step', total=self.noise_scheduler.num_timesteps):\n            times = torch.full((batch,), i, device = device, dtype = torch.long)\n            self_cond = x_start if self.net.self_cond else None\n            image_embed, x_start = self.p_sample(image_embed, times, text_cond = text_cond, self_cond = self_cond, cond_scale = cond_scale)\n        if self.sampling_final_clamp_l2norm and self.predict_x_start:\n            image_embed = self.l2norm_clamp_embed(image_embed)\n        return image_embed\n    @torch.no_grad()\n    def p_sample_loop_ddim(self, shape, text_cond, *, timesteps, eta = 1., cond_scale = 1.):\n        batch, device, alphas, total_timesteps = shape[0], self.device, self.noise_scheduler.alphas_cumprod_prev, self.noise_scheduler.num_timesteps\n        times = torch.linspace(-1., total_timesteps, steps = timesteps + 1)[:-1]\n        times = list(reversed(times.int().tolist()))\n        time_pairs = list(zip(times[:-1], times[1:]))\n        image_embed = torch.randn(shape, device = device)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1296-1316"
    },
    "367": {
        "file_id": 10,
        "content": "The code defines the `p_sample` function which samples images and their corresponding embeddings using a loop over time steps. It also includes an optional L2-norm clamping for final image embedding. The `p_sample_loop_ddim` function is a helper method to define shape, times, and time pairs for the sampling loop in DDIM style.",
        "type": "comment"
    },
    "368": {
        "file_id": 10,
        "content": "        x_start = None # for self-conditioning\n        if self.init_image_embed_l2norm:\n            image_embed = l2norm(image_embed) * self.image_embed_scale\n        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n            alpha = alphas[time]\n            alpha_next = alphas[time_next]\n            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n            self_cond = x_start if self.net.self_cond else None\n            pred = self.net.forward_with_cond_scale(image_embed, time_cond, self_cond = self_cond, cond_scale = cond_scale, **text_cond)\n            # derive x0\n            if self.predict_v:\n                x_start = self.noise_scheduler.predict_start_from_v(image_embed, t = time_cond, v = pred)\n            elif self.predict_x_start:\n                x_start = pred\n            else:\n                x_start = self.noise_scheduler.predict_start_from_noise(image_embed, t = time_cond, noise = pred)\n            # clip x0 before maybe predicting noise",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1318-1342"
    },
    "369": {
        "file_id": 10,
        "content": "The code is iterating through time pairs, calculating alpha values and performing a forward pass in the neural network. It also adjusts x_start based on prediction methods and performs noise scheduling. The purpose seems to be generating an image using conditional sampling with self-conditioning and considering different prediction methods for x_start.",
        "type": "comment"
    },
    "370": {
        "file_id": 10,
        "content": "            if not self.predict_x_start:\n                x_start.clamp_(-1., 1.)\n            if self.predict_x_start and self.sampling_clamp_l2norm:\n                x_start = self.l2norm_clamp_embed(x_start)\n            # predict noise\n            pred_noise = self.noise_scheduler.predict_noise_from_start(image_embed, t = time_cond, x0 = x_start)\n            if time_next < 0:\n                image_embed = x_start\n                continue\n            c1 = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n            c2 = ((1 - alpha_next) - torch.square(c1)).sqrt()\n            noise = torch.randn_like(image_embed) if time_next > 0 else 0.\n            image_embed = x_start * alpha_next.sqrt() + \\\n                          c1 * noise + \\\n                          c2 * pred_noise\n        if self.predict_x_start and self.sampling_final_clamp_l2norm:\n            image_embed = self.l2norm_clamp_embed(image_embed)\n        return image_embed\n    @torch.no_grad()\n    def p_sample_loop(self, *args, timesteps = None, **kwargs):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1344-1372"
    },
    "371": {
        "file_id": 10,
        "content": "In this code segment, it checks if predicting x_start is enabled and performs L2-norm clamping if necessary. It then predicts noise using the noise scheduler based on image embeddings, time condition, and x_start. If time_next is less than 0, it sets image_embed to x_start. Calculates coefficients c1 and c2 for RNN sampling and generates noise accordingly. Combines these elements to generate the final image_embed which is then optionally L2-norm clamped if enabled.",
        "type": "comment"
    },
    "372": {
        "file_id": 10,
        "content": "        timesteps = default(timesteps, self.noise_scheduler.num_timesteps)\n        assert timesteps <= self.noise_scheduler.num_timesteps\n        is_ddim = timesteps < self.noise_scheduler.num_timesteps\n        if not is_ddim:\n            normalized_image_embed = self.p_sample_loop_ddpm(*args, **kwargs)\n        else:\n            normalized_image_embed = self.p_sample_loop_ddim(*args, **kwargs, timesteps = timesteps)\n        image_embed = normalized_image_embed / self.image_embed_scale\n        return image_embed\n    def p_losses(self, image_embed, times, text_cond, noise = None):\n        noise = default(noise, lambda: torch.randn_like(image_embed))\n        image_embed_noisy = self.noise_scheduler.q_sample(x_start = image_embed, t = times, noise = noise)\n        self_cond = None\n        if self.net.self_cond and random.random() < 0.5:\n            with torch.no_grad():\n                self_cond = self.net(image_embed_noisy, times, **text_cond).detach()\n        pred = self.net(\n            image_embed_noisy,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1373-1396"
    },
    "373": {
        "file_id": 10,
        "content": "This code is from the DALLE2-pytorch model. It first determines if the timesteps are less than the number of timesteps in the noise scheduler. If so, it uses the p_sample_loop_ddim function to get the normalized image embeddings, otherwise it uses the p_sample_loop_ddpm function. The code then scales the normalized image embeddings by the image_embed_scale and returns the scaled embeddings. The p_losses function generates a noisy version of the input image embedding using the noise scheduler, and optionally conditions the model with self-conditioning if the condition is met. Finally, it passes the noisy embedding to the network for prediction.",
        "type": "comment"
    },
    "374": {
        "file_id": 10,
        "content": "            times,\n            self_cond = self_cond,\n            text_cond_drop_prob = self.text_cond_drop_prob,\n            image_cond_drop_prob = self.image_cond_drop_prob,\n            **text_cond\n        )\n        if self.predict_x_start and self.training_clamp_l2norm:\n            pred = self.l2norm_clamp_embed(pred)\n        if self.predict_v:\n            target = self.noise_scheduler.calculate_v(image_embed, times, noise)\n        elif self.predict_x_start:\n            target = image_embed\n        else:\n            target = noise\n        loss = self.noise_scheduler.loss_fn(pred, target)\n        return loss\n    @torch.no_grad()\n    @eval_decorator\n    def sample_batch_size(self, batch_size, text_cond, cond_scale = 1.):\n        device = self.betas.device\n        shape = (batch_size, self.image_embed_dim)\n        img = torch.randn(shape, device = device)\n        for i in tqdm(reversed(range(0, self.noise_scheduler.num_timesteps)), desc = 'sampling loop time step', total = self.noise_scheduler.num_timesteps):",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1397-1425"
    },
    "375": {
        "file_id": 10,
        "content": "The code defines a method for predicting and calculating loss. It takes in parameters such as times, self_cond, text_cond_drop_prob, image_cond_drop_prob, and text_cond. If certain conditions are met, it performs l2norm clamping on the prediction, sets the target based on whether to predict x or v, then calculates the loss using the noise scheduler's loss function. The code also includes a sample_batch_size method that samples an image batch and iterates over time steps in reverse order for some processing.",
        "type": "comment"
    },
    "376": {
        "file_id": 10,
        "content": "            img = self.p_sample(img, torch.full((batch_size,), i, device = device, dtype = torch.long), text_cond = text_cond, cond_scale = cond_scale)\n        return img\n    @torch.no_grad()\n    @eval_decorator\n    def sample(\n        self,\n        text,\n        num_samples_per_batch = 2,\n        cond_scale = 1.,\n        timesteps = None\n    ):\n        timesteps = default(timesteps, self.sample_timesteps)\n        # in the paper, what they did was\n        # sample 2 image embeddings, choose the top 1 similarity, as judged by CLIP\n        text = repeat(text, 'b ... -> (b r) ...', r = num_samples_per_batch)\n        batch_size = text.shape[0]\n        image_embed_dim = self.image_embed_dim\n        text_embed, text_encodings = self.clip.embed_text(text)\n        text_cond = dict(text_embed = text_embed)\n        if self.condition_on_text_encodings:\n            text_cond = {**text_cond, 'text_encodings': text_encodings}\n        image_embeds = self.p_sample_loop((batch_size, image_embed_dim), text_cond = text_cond, cond_scale = cond_scale, timesteps = timesteps)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1426-1454"
    },
    "377": {
        "file_id": 10,
        "content": "This code is part of a DALL-E 2 model implementation in PyTorch. The sample function generates multiple image embeddings based on provided text, then chooses the most similar one according to CLIP's similarity judgment. The function uses a p_sample_loop method which takes timesteps as input and returns a batch of images with the specified size.",
        "type": "comment"
    },
    "378": {
        "file_id": 10,
        "content": "        # retrieve original unscaled image embed\n        text_embeds = text_cond['text_embed']\n        text_embeds = rearrange(text_embeds, '(b r) d -> b r d', r = num_samples_per_batch)\n        image_embeds = rearrange(image_embeds, '(b r) d -> b r d', r = num_samples_per_batch)\n        text_image_sims = einsum('b r d, b r d -> b r', l2norm(text_embeds), l2norm(image_embeds))\n        top_sim_indices = text_image_sims.topk(k = 1).indices\n        top_sim_indices = repeat(top_sim_indices, 'b 1 -> b 1 d', d = image_embed_dim)\n        top_image_embeds = image_embeds.gather(1, top_sim_indices)\n        return rearrange(top_image_embeds, 'b 1 d -> b d')\n    def forward(\n        self,\n        text = None,\n        image = None,\n        text_embed = None,      # allow for training on preprocessed CLIP text and image embeddings\n        image_embed = None,\n        text_encodings = None,  # as well as CLIP text encodings\n        *args,\n        **kwargs\n    ):\n        assert exists(text) ^ exists(text_embed), 'either text or text embedding must be supplied'",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1456-1481"
    },
    "379": {
        "file_id": 10,
        "content": "This function retrieves the original unscaled image embeddings from the input, rearranges them based on the number of samples per batch, calculates text-image similarities using Euclidean distance, gets the top indices and gathers corresponding embeddings. It allows for training on preprocessed CLIP text and image embeddings or CLIP text encodings. If neither text nor text embedding is supplied, an assertion error will be raised.",
        "type": "comment"
    },
    "380": {
        "file_id": 10,
        "content": "        assert exists(image) ^ exists(image_embed), 'either image or image embedding must be supplied'\n        assert not (self.condition_on_text_encodings and (not exists(text_encodings) and not exists(text))), 'text encodings must be present if you specified you wish to condition on it on initialization'\n        if exists(image):\n            image_embed, _ = self.clip.embed_image(image)\n        # calculate text conditionings, based on what is passed in\n        if exists(text):\n            text_embed, text_encodings = self.clip.embed_text(text)\n        text_cond = dict(text_embed = text_embed)\n        if self.condition_on_text_encodings:\n            assert exists(text_encodings), 'text encodings must be present for diffusion prior if specified'\n            text_cond = {**text_cond, 'text_encodings': text_encodings}\n        # timestep conditioning from ddpm\n        batch, device = image_embed.shape[0], image_embed.device\n        times = self.noise_scheduler.sample_random_times(batch)\n        # scale image embed (Katherine)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1482-1504"
    },
    "381": {
        "file_id": 10,
        "content": "The code snippet checks if an image or image embedding is supplied and throws an error if neither exists. It also verifies the presence of text encodings or text based on the specified conditioning during initialization. The code then calculates the text embeddings from the given text using the clip model. If conditioned on text encodings, it includes them in the text_cond dictionary. It samples random times for timestep conditioning from the noise scheduler and scales the image embed (by Katherine).",
        "type": "comment"
    },
    "382": {
        "file_id": 10,
        "content": "        image_embed *= self.image_embed_scale\n        # calculate forward loss\n        return self.p_losses(image_embed, times, text_cond = text_cond, *args, **kwargs)\n# decoder\ndef NearestUpsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n        self.init_conv_(conv)\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1506-1543"
    },
    "383": {
        "file_id": 10,
        "content": "This code contains two classes, `NearestUpsample` and `PixelShuffleUpsample`. `NearestUpsample` performs nearest neighbor upsampling followed by a convolution operation. `PixelShuffleUpsample` applies pixel shuffling after a 1x1 convolution to reduce checkerboard artifacts. Both classes can be used for image upsampling tasks.",
        "type": "comment"
    },
    "384": {
        "file_id": 10,
        "content": "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n    def forward(self, x):\n        return self.net(x)\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\nclass WeightStandardizedConv2d(nn.Conv2d):\n    \"\"\"\n    https://arxiv.org/abs/1903.10520\n    weight standardization purportedly works synergistically with group normalization\n    \"\"\"\n    def forward(self, x):\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        weight = self.weight\n        flattened_weights = rearrange(weight, 'o ... -> o (...)')\n        mean = reduce(weight, 'o ... -> o 1 1 1', 'mean')\n        var = torch.var(flattened_weights, dim = -1, unbiased = False)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1544-1574"
    },
    "385": {
        "file_id": 10,
        "content": "The code defines a class called WeightStandardizedConv2d that extends nn.Conv2d and implements weight standardization for improving synergy with group normalization. It also includes a function named Downsample to downsample the input using pixel unshuffle technique, which is optimal according to a reference paper. The forward method in WeightStandardizedConv2d calculates mean and variance of flattened weights and performs weight standardization before applying convolution operations.",
        "type": "comment"
    },
    "386": {
        "file_id": 10,
        "content": "        var = rearrange(var, 'o -> o 1 1 1')\n        weight = (weight - mean) * (var + eps).rsqrt()\n        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n    def forward(self, x):\n        dtype, device = x.dtype, x.device\n        assert is_float_dtype(dtype), 'input to sinusoidal pos emb must be a float type'\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = device, dtype = dtype) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1).type(dtype)\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        weight_standardization = False\n    ):\n        super().__init__()\n        conv_klass = nn.Conv2d if not weight_standardization else WeightStandardizedConv2d",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1575-1605"
    },
    "387": {
        "file_id": 10,
        "content": "This code snippet contains the definition of three classes: `rearrange`, `SinusoidalPosEmb`, and `Block`. The `rearrange` function is used to reshape tensors, `SinusoidalPosEmb` class computes sinusoidal positional embeddings, and `Block` class defines a convolutional block with an option for weight standardization.",
        "type": "comment"
    },
    "388": {
        "file_id": 10,
        "content": "        self.project = conv_klass(dim, dim_out, 3, padding = 1)\n        self.norm = nn.GroupNorm(groups, dim_out)\n        self.act = nn.SiLU()\n    def forward(self, x, scale_shift = None):\n        x = self.project(x)\n        x = self.norm(x)\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n        x = self.act(x)\n        return x\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        weight_standardization = False,\n        cosine_sim_cross_attn = False\n    ):\n        super().__init__()\n        self.time_mlp = None\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n        self.cross_attn = None\n        if exists(cond_dim):\n            self.cross_attn = CrossAttention(\n                dim = dim_out,\n                context_dim = cond_dim,",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1607-1649"
    },
    "389": {
        "file_id": 10,
        "content": "This code snippet defines a ResnetBlock class that takes in dimensions and other parameters for its initialization. It includes a project layer, normalization layer, activation function, and optional scale-shift operation. The forward method performs the computation steps involving these layers. Additionally, it checks if time_cond_dim is given to initialize a time MLP and if cond_dim exists to initialize a cross-attention layer.",
        "type": "comment"
    },
    "390": {
        "file_id": 10,
        "content": "                cosine_sim = cosine_sim_cross_attn\n            )\n        self.block1 = Block(dim, dim_out, groups = groups, weight_standardization = weight_standardization)\n        self.block2 = Block(dim_out, dim_out, groups = groups, weight_standardization = weight_standardization)\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n    def forward(self, x, time_emb = None, cond = None):\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n        h = self.block1(x, scale_shift = scale_shift)\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c ... -> b ... c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b ... c -> b c ...')",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1650-1676"
    },
    "391": {
        "file_id": 10,
        "content": "This code defines a class for an encoder-decoder architecture with residual connections and cross-attention. It includes blocks, convolutions, time MLP, and optional cross-attention with conditional input. The forward method processes the input, applies blocks, optionally performs time embedding and cross-attention, and returns the output.",
        "type": "comment"
    },
    "392": {
        "file_id": 10,
        "content": "        h = self.block2(h)\n        return h + self.res_conv(x)\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.,\n        norm_context = False,\n        cosine_sim = False,\n        cosine_sim_scale = 16\n    ):\n        super().__init__()\n        self.cosine_sim = cosine_sim\n        self.scale = cosine_sim_scale if cosine_sim else (dim_head ** -0.5)\n        self.heads = heads\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, dim)\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else nn.Identity()\n        self.dropout = nn.Dropout(dropout)\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1678-1711"
    },
    "393": {
        "file_id": 10,
        "content": "The code defines a CrossAttention class with parameters for dimensionality, context dimension, number of heads, dropout rate, and normalization options. It initializes the necessary layers including linear transformations and layer norms. The cosine similarity scale and null keys are also defined.",
        "type": "comment"
    },
    "394": {
        "file_id": 10,
        "content": "            LayerNorm(dim)\n        )\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n        x = self.norm(x)\n        context = self.norm_context(context)\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n        # add null key / value for classifier free guidance in prior net\n        nk, nv = map(lambda t: repeat(t, 'd -> b h 1 d', h = self.heads,  b = b), self.null_kv.unbind(dim = -2))\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n        if self.cosine_sim:\n            q, k = map(l2norm, (q, k))\n        q, k = map(lambda t: t * math.sqrt(self.scale), (q, k))\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n        max_neg_value = -torch.finfo(sim.dtype).max\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1712-1743"
    },
    "395": {
        "file_id": 10,
        "content": "This function defines a multi-head attention layer. It normalizes input x and context, splits them into queries (q), keys (k), and values (v). It also includes null key/value pairs for classifier free guidance in the prior net. If cosine_sim is set, it normalizes q and k again. It then computes the attention scores (sim) between q and k, and applies a mask if available, replacing negative values with max_neg_value.",
        "type": "comment"
    },
    "396": {
        "file_id": 10,
        "content": "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.type(sim.dtype)\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n        self.nonlin = nn.GELU()\n        self.to_qkv = nn.Conv2d(dim, inner_dim * 3, 1, bias = False)\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n    def forward(self, fmap):\n        h, x, y = self.heads, *fmap.shape[-2:]\n        seq_len = x * y\n        fmap = self.norm(fmap)\n        q, k, v = self.to_qkv(fmap).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = h), (q, k, v))",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1745-1780"
    },
    "397": {
        "file_id": 10,
        "content": "This code defines a LinearAttention module that performs multi-head attention. It normalizes the input, applies convolutions to split input into queries (Q), keys (K), and values (V), then computes attention weights, rearranges output dimensions for efficiency, and finally passes the result through another set of convolutions before returning it.",
        "type": "comment"
    },
    "398": {
        "file_id": 10,
        "content": "        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n        q = q * self.scale\n        v = l2norm(v)\n        k, v = map(lambda t: t / math.sqrt(seq_len), (k, v))\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n        out = self.nonlin(out)\n        return self.to_out(out)\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n        self.convs = nn.ModuleList([])",
        "type": "code",
        "location": "/dalle2_pytorch/dalle2_pytorch.py:1782-1816"
    },
    "399": {
        "file_id": 10,
        "content": "The code calculates and applies attention weights to query (q) and key (k) tensors, normalizes them, scales the vectors, and performs element-wise multiplication. It then applies a linear transformation (nonlin) on the result and rearranges the dimensions of the output tensor using the 'rearrange' function. The code also defines a CrossEmbedLayer class that initializes convolutional layers for feature extraction at multiple scales.",
        "type": "comment"
    }
}